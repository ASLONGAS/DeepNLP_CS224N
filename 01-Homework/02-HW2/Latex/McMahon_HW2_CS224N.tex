\documentclass[fleqn]{MJD}

\usepackage{cancel}
\usepackage{cleveref}
\usepackage{titlesec}
\usepackage{hyperref}
%\colorsections
%\bluelinks
\newcommand{\problem}[1]{\chapter{Problem #1}}
\newcommand{\subproblem}[2]{\section{(#1)~ #2}}
\newcommand{\subsubproblem}[2]{\subsection{ #1)~ #2}}
\newcommand{\U}{\cup}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\s}{\subset}
\renewcommand{\equiv}{\Leftrightarrow}
\newcommand{\0}{\emptyset}
\newcommand{\imp}{\Rightarrow}
\newcommand{\Usum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\intsum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\infsum}{\sum\limits_{i=1}^\infty}
\newcommand{\sets}{\{A_1, A_2 \dots\} }
\newcommand{\nsets}{\{A_1, \dots, A_n \} }

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\LARGE}
  
\graphicspath{ {../Code/} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\titleAT[CS 224N: Assignment 2]{Ryan McMahon}
\large

\begingroup
\let\clearpage\relax
\tableofcontents
\endgroup
\newpage

%-------------------------------------
\problem{1: Tensorflow Softmax (25 pts)}
%-------------------------------------
\textit{In this question, we will implement a linear classifier with loss function}

\begin{align}
	J(\bm{W}) &= CE(\bm{y}, softmax(\bm{xW}))
\end{align}

\noindent \textit{Where $\bm{x}$ is a row vector of features and $\bm{W}$ is the weight matrix for the model. We will use TensorFlow's automatic differentiation capability to fit this model to provided data.}

\vskip2em

%----------------------
\subproblem{a}{Softmax in Tensorflow (5 pts)}
\textit{Implement the softmax function using TensorFlow in }\verb|q1_softmax.py|. \textit{Remember that} 

\begin{align}
	softmax(\bm{x})_{i} &= \frac{e^{\bm{x}_{i}}}{\sum_{j} e^{\bm{x}_{j}}} 
\end{align}

\noindent \textit{Note that you may not use }\verb|tf.nn.softmax| \textit{or related built-in functions. You can run basic (nonexhaustive tests) by running }\verb|python q1_softmax.py|. \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.

\vskip5em




%----------------------
\subproblem{b}{Cross-Entropy Loss in Tensorflow (5 pts)}
\textit{Implement the cross-entropy loss using TensorFlow in }\verb|q1_softmax.py|. \textit{Remember that}

\begin{align}
	CE(\bm{y},\hat{\bm{y}}) &= - \sum_{i=1}^{N_{c}} y_{i} log(\hat{y}_{i})
\end{align}

\noindent \textit{where $\bm{y} \in \mathbb{R}^{N_{c}}$ is a one-hot label vector and $N_{c}$ is the number of classes. This loss is summed over all examples (rows) of a minibatch. Note that you may \textbf{not} use TensorFlowâ€™s built-in cross-entropy functions for this question. You can run basic (non-exhaustive tests) by running }\verb|python q1_softmax.py|. \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.




\newpage
%----------------------
\subproblem{c}{Placeholders and Feed Dictionaries (5 pts)}
\textit{Carefully study the Model class in }\verb|model.py|. \textit{Briefly explain the purpose of placeholder variables and feed dictionaries in TensorFlow computations. Fill in the implementations for} \verb|add_placeholders|\textit{ and} \verb|create_feed_dict| \textit{in} \verb|q1_classifier.py|.\vskip1em
\noindent \textit{\textbf{Hint:} Note that configuration variables are stored in the }\verb|Config| \textit{class. You will need to use these configurations variables in the code.} \vskip2em



\noindent \textbf{Answer:} \\

\noindent 

\end{document}
