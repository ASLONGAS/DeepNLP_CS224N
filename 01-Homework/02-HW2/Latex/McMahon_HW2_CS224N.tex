\documentclass[fleqn]{MJD}

\usepackage{cancel}
\usepackage{cleveref}
\usepackage{titlesec}
\usepackage{hyperref}
%\colorsections
%\bluelinks
\newcommand{\problem}[1]{\chapter{Problem #1}}
\newcommand{\subproblem}[2]{\section{(#1)~ #2}}
\newcommand{\subsubproblem}[2]{\subsection{ #1)~ #2}}
\newcommand{\U}{\cup}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\s}{\subset}
\renewcommand{\equiv}{\Leftrightarrow}
\newcommand{\0}{\emptyset}
\newcommand{\imp}{\Rightarrow}
\newcommand{\Usum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\intsum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\infsum}{\sum\limits_{i=1}^\infty}
\newcommand{\sets}{\{A_1, A_2 \dots\} }
\newcommand{\nsets}{\{A_1, \dots, A_n \} }

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\LARGE}
  
\graphicspath{ {../Code/} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\titleAT[CS 224N: Assignment 2]{Ryan McMahon}
\large

\begingroup
\let\clearpage\relax
\tableofcontents
\endgroup
\newpage

%-------------------------------------
\problem{1: Tensorflow Softmax (25 pts)}
%-------------------------------------
\textit{In this question, we will implement a linear classifier with loss function}

\begin{align}
	J(\bm{W}) &= CE(\bm{y}, softmax(\bm{xW}))
\end{align}

\noindent \textit{Where $\bm{x}$ is a row vector of features and $\bm{W}$ is the weight matrix for the model. We will use TensorFlow's automatic differentiation capability to fit this model to provided data.}

\vskip2em

%----------------------
\subproblem{a}{Softmax in Tensorflow (5 pts)}
\textit{Implement the softmax function using TensorFlow in }\verb|q1_softmax.py|. \textit{Remember that} 

\begin{align}
	softmax(\bm{x})_{i} &= \frac{e^{\bm{x}_{i}}}{\sum_{j} e^{\bm{x}_{j}}} 
\end{align}

\noindent \textit{Note that you may not use }\verb|tf.nn.softmax| \textit{or related built-in functions. You can run basic (nonexhaustive tests) by running }\verb|python q1_softmax.py|. \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.

\vskip5em




%----------------------
\subproblem{b}{Cross-Entropy Loss in Tensorflow (5 pts)}
\textit{Implement the cross-entropy loss using TensorFlow in }\verb|q1_softmax.py|. \textit{Remember that}

\begin{align}
	CE(\bm{y},\hat{\bm{y}}) &= - \sum_{i=1}^{N_{c}} y_{i} log(\hat{y}_{i})
\end{align}

\noindent \textit{where $\bm{y} \in \mathbb{R}^{N_{c}}$ is a one-hot label vector and $N_{c}$ is the number of classes. This loss is summed over all examples (rows) of a minibatch. Note that you may \textbf{not} use TensorFlow’s built-in cross-entropy functions for this question. You can run basic (non-exhaustive tests) by running }\verb|python q1_softmax.py|. \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.




\newpage
%----------------------
\subproblem{c}{Placeholders and Feed Dictionaries (5 pts)}
\textit{Carefully study the Model class in }\verb|model.py|. \textit{Briefly explain the purpose of placeholder variables and feed dictionaries in TensorFlow computations. Fill in the implementations for} \verb|add_placeholders|\textit{ and} \verb|create_feed_dict| \textit{in} \verb|q1_classifier.py|.
%
\vskip1em
%
\noindent \textit{\textbf{Hint:} Note that configuration variables are stored in the }\verb|Config| \textit{class. You will need to use these configurations variables in the code.} 
%
\vskip2em



\noindent \textbf{Answer:} \\

\noindent The purpose of placeholder variables and feed dictionaries is, in short, to improve computational efficiency. They allow us to specify the graph structure without supplying the actual data. By doing this, our model can be flexible to different training approaches (e.g., different mini-batch sizes). Specifically, the placeholders stand in for whatever data/values we want to use and the feed dictionaries tell the placeholders what data/values to use. \\

\noindent Also, see code: $\sim$\verb|/code/q1_classifier.py|.

\vskip5em




%----------------------
\subproblem{d}{Add Softmax and  Add CE Loss (5 pts)}
\textit{Implement the transformation for a softmax classifier in the function }\verb|add_prediction_op|\textit{ in }\verb|q1_classifier.py|. \textit{Add cross-entropy loss in the function }\verb|add_loss_op| \textit{in the same file. Use the implementations from the earlier parts of the problem, \textbf{not} TensorFlow built-ins.} \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_classifier.py|.


\vskip5em




%----------------------
\subproblem{e}{Add Training Optimizer -- Gradient Descent (5 pts)}
\textit{Fill in the implementation for }\verb|add_training_op| \textit{in }\verb|q1_classifier.py|. \textit{Explain how TensorFlow’s automatic differentiation removes the need for us to define gradients explicitly. Verify that your model is able to fit to synthetic data by running }\verb|q1_classifier.py| \textit{and making sure that the tests pass.}
%
\vskip1em
%
\noindent \textit{\textbf{Hint:} Make sure to use the learning rate specified in }\verb|Config|.
%
\vskip2em

\noindent \textbf{Answer:} \\

\noindent 




\end{document}
