\documentclass[fleqn]{MJD}

\usepackage{cancel}
\usepackage{cleveref}
\usepackage{titlesec}
\usepackage{hyperref}
%\colorsections
%\bluelinks
\newcommand{\problem}[1]{\chapter{Problem #1}}
\newcommand{\subproblem}[2]{\section{(#1)~ #2}}
\newcommand{\subsubproblem}[2]{\subsection{ #1)~ #2}}
\newcommand{\U}{\cup}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\s}{\subset}
\renewcommand{\equiv}{\Leftrightarrow}
\newcommand{\0}{\emptyset}
\newcommand{\imp}{\Rightarrow}
\newcommand{\Usum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\intsum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\infsum}{\sum\limits_{i=1}^\infty}
\newcommand{\sets}{\{A_1, A_2 \dots\} }
\newcommand{\nsets}{\{A_1, \dots, A_n \} }

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\LARGE}
  
\graphicspath{ {../Code/} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\titleAT[CS 224N: Assignment 2]{Ryan McMahon}
\large

\begingroup
\let\clearpage\relax
\tableofcontents
\endgroup
\newpage

%-------------------------------------
\problem{1: Tensorflow Softmax (25 pts)}
%-------------------------------------
\textit{In this question, we will implement a linear classifier with loss function}

\begin{align}
	J(\bm{W}) &= CE(\bm{y}, softmax(\bm{xW}))
\end{align}

\noindent \textit{Where $\bm{x}$ is a row vector of features and $\bm{W}$ is the weight matrix for the model. We will use TensorFlow's automatic differentiation capability to fit this model to provided data.}

\vskip2em

%----------------------
\subproblem{a}{Softmax in Tensorflow (5 pts)}
\textit{Implement the softmax function using TensorFlow in }\verb|q1_softmax.py|. \textit{Remember that} 

\begin{align}
	softmax(\bm{x})_{i} &= \frac{e^{\bm{x}_{i}}}{\sum_{j} e^{\bm{x}_{j}}} 
\end{align}

\noindent \textit{Note that you may not use }\verb|tf.nn.softmax| \textit{or related built-in functions. You can run basic (nonexhaustive tests) by running }\verb|python q1_softmax.py|. \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.

\vskip5em




%----------------------
\subproblem{b}{Cross-Entropy Loss in Tensorflow (5 pts)}
\textit{Implement the cross-entropy loss using TensorFlow in }\verb|q1_softmax.py|. \textit{Remember that}

\begin{align}
	CE(\bm{y},\hat{\bm{y}}) &= - \sum_{i=1}^{N_{c}} y_{i} log(\hat{y}_{i})
\end{align}

\noindent \textit{where $\bm{y} \in \mathbb{R}^{N_{c}}$ is a one-hot label vector and $N_{c}$ is the number of classes. This loss is summed over all examples (rows) of a minibatch. Note that you may \textbf{not} use TensorFlow’s built-in cross-entropy functions for this question. You can run basic (non-exhaustive tests) by running }\verb|python q1_softmax.py|. \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.




\newpage
%----------------------
\subproblem{c}{Placeholders and Feed Dictionaries (5 pts)}
\textit{Carefully study the Model class in }\verb|model.py|. \textit{Briefly explain the purpose of placeholder variables and feed dictionaries in TensorFlow computations. Fill in the implementations for} \verb|add_placeholders|\textit{ and} \verb|create_feed_dict| \textit{in} \verb|q1_classifier.py|.
%
\vskip1em
%
\noindent \textit{\textbf{Hint:} Note that configuration variables are stored in the }\verb|Config| \textit{class. You will need to use these configurations variables in the code.} 
%
\vskip2em



\noindent \textbf{Answer:} \\

\noindent The purpose of placeholder variables and feed dictionaries is, in short, to improve computational efficiency. They allow us to specify the graph structure without supplying the actual data. By doing this, our model can be flexible to different training approaches (e.g., different mini-batch sizes). Specifically, the placeholders stand in for whatever data/values we want to use and the feed dictionaries tell the placeholders what data/values to use. \\

\noindent Also, see code: $\sim$\verb|/code/q1_classifier.py|.

\vskip5em




%----------------------
\subproblem{d}{Add Softmax and  Add CE Loss (5 pts)}
\textit{Implement the transformation for a softmax classifier in the function }\verb|add_prediction_op|\textit{ in }\verb|q1_classifier.py|. \textit{Add cross-entropy loss in the function }\verb|add_loss_op| \textit{in the same file. Use the implementations from the earlier parts of the problem, \textbf{not} TensorFlow built-ins.} \\


\noindent \textbf{Answer:}\\

\noindent See code: $\sim$\verb|/code/q1_classifier.py|.




\newpage

%----------------------
\subproblem{e}{Add Training Optimizer -- Gradient Descent (5 pts)}
\textit{Fill in the implementation for }\verb|add_training_op| \textit{in }\verb|q1_classifier.py|. \textit{Explain how TensorFlow’s automatic differentiation removes the need for us to define gradients explicitly. Verify that your model is able to fit to synthetic data by running }\verb|q1_classifier.py| \textit{and making sure that the tests pass.}
%
\vskip1em
%
\noindent \textit{\textbf{Hint:} Make sure to use the learning rate specified in }\verb|Config|.
%
\vskip2em

\noindent \textbf{Answer:} \\

\noindent TensorFlow's automatic differentiation works by representing a given model as a symbolic Directed Acyclic Graph (DAG). Using this DAG, we can break down an arbitrarily complex function into basic mathematical operations (e.g., $+$, $-$, $\times$, $\div$, $log$, $\dots$ etc.). Once this is done, TensorFlow is able to numerically calculate the necessary partial derivatives for the gradients using the chain-rule, which alleviates the need for us to define them ourselves. \\


\noindent Also, see code: $\sim$\verb|/code/q1_classifier.py|.





\newpage

%-------------------------------------
\problem{2: Neural Transition-Based Dependency Parsing (50 pts)}
%-------------------------------------
\textit{In this section, you’ll be implementing a neural-network based dependency parser. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between ``head'' words and words which modify those heads. Your implementation will be a \textbf{transition-based} parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:} 

\begin{itemize}
	\item \textit{A \textbf{stack} of words that are currently being processed.}
	\item \textit{A \textbf{buffer} of words yet to be processed.}
	\item \textit{A list of \textbf{dependencies} predicted by the parser.}
\end{itemize}

\noindent \textit{Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words of the sentence in order. At each step, the parse applies a \textbf{transition} to the partial parse until its buffer is empty and the stack is of size 1. The following transitions can be applied:}

\begin{itemize}
	\item \verb|SHIFT|: \textit{removes the first word from the buffer and pushes it onto the stack.}
	\item \verb|LEFT-ARC|: \textit{marks the second (second most recently added) item on the stack as a dependent of the	first item and removes the second item from the stack.}
	\item \verb|RIGHT-ARC|: \textit{marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack}
\end{itemize}

\noindent \textit{Your parser will decide among transitions at each state using a neural network classifier. First, you will implement the partial parse representation and transition functions.} 



\newpage

%----------------------
\subproblem{a}{Parsing Steps by Hand (6 pts)}
\textit{Go through the sequence of transitions needed for parsing the sentence ``\textbf{I parsed this sentence correctly}''. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example.} \\

\noindent \textbf{NOTE: See assignment for table and tree.}  
\vskip2em


\noindent \textbf{Answer:} \\

\begin{table}[!htbp]
	\centering
	\small
\begin{tabular}{|l|l|l|l|}
									&											& \textbf{new}			&						\\
\textbf{stack} 						& \textbf{buffer}							& \textbf{dependency}	& \textbf{transition} 	\\ \toprule
$[ROOT]$ 							& $[I, parsed, this, sentence, correctly]$  &  							& Initial Configuration \\ 
$[ROOT, I]$ 						& $[parsed, this, sentence, correctly]$ 	&  							& \verb|SHIFT|  		\\ 
$[ROOT, I, parsed]$ 				& $[this, sentence, correctly]$ 			&  							& \verb|SHIFT|  		\\ 
$[ROOT, parsed]$ 					& $[this, sentence, correctly]$ 			& parsed $\rightarrow$ I 	& \verb|LEFT-ARC| 		\\ 
$[ROOT, parsed, this]$				& $[sentence, correctly]$ 					&  							& \verb|SHIFT|  		\\ 
$[ROOT, parsed, this, sentence]$	& $[correctly]$ 							&   						& \verb|SHIFT| 			\\ 
$[ROOT, parsed, sentence]$ 			& $[correctly]$ 						& sentence $\rightarrow$ this	& \verb|LEFT-ARC| 		\\ 
$[ROOT, parsed]$ 					& $[correctly]$ 						& parsed $\rightarrow$ sentence	& \verb|RIGHT-ARC| 		\\ 
$[ROOT, parsed, correctly]$			& $[ ]$ 									&   						& \verb|SHIFT| 			\\ 
$[ROOT, parsed]$ 					& $[ ]$ 								& parsed $\rightarrow$ correctly& \verb|RIGHT-ARC| 		\\ 
$[ROOT]$ 							& $[ ]$ 									& ROOT $\rightarrow$ parsed & \verb|RIGHT-ARC| 		\\
\end{tabular} 
\end{table}

\vskip5em



%----------------------
\subproblem{b}{Number of Steps to Parse (2 pts)}
\textit{A sentence containing $n$ words will be parsed in how many steps (in terms of $n$)? Briefly explain why.}\\

\noindent \textbf{Answer:} \\

\noindent Assuming we have a grammatically well-formed sentence and need a first step from \verb|ROOT|:
\begin{align}
	n+2 \le \normalfont{Steps} \le n^{3} \nonumber
\end{align}

\noindent The lower bound exists because we need: 1) an arc from \verb|ROOT| to the sentence head, and 2) an arc between a subject and a verb. The $n$ is derived from all of the necessary shifts. Our upper bound comes from Eisner (1996) -- whose algorithm allows sub-sequences to be parsed independently of one another. 




\newpage 

%----------------------
\subproblem{c}{Partial Parse Coding (6 pts)}
\textit{Implement the} \verb|__init__| \textit{and} \verb|parse_step| \textit{functions in the} \verb|PartialParse| \textit{class in} \verb|q2_parser_transitions.py|. \textit{This implements the transition mechanics your parser will use. You can run basic (not-exhaustive) tests by running} \verb|python q2_parser_transitions.py|. \\ 


\noindent \textbf{Answer:} \\

\noindent See code: $\sim$\verb|/code/q2_parser_transitions.py|.







\end{document}
