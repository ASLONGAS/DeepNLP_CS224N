\documentclass[fleqn]{MJD}

\usepackage{cancel}
\usepackage{cleveref}
\usepackage{titlesec}
%\colorsections
%\bluelinks
\newcommand{\problem}[1]{\chapter{Problem #1}}
\newcommand{\subproblem}[2]{\section{(#1)~ #2}}
\newcommand{\subsubproblem}[2]{\subsection{ #1)~ #2}}
\newcommand{\U}{\cup}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\s}{\subset}
\renewcommand{\equiv}{\Leftrightarrow}
\newcommand{\0}{\emptyset}
\newcommand{\imp}{\Rightarrow}
\newcommand{\Usum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\intsum}{\bigcup\limits_{i=1}^\infty}
\newcommand{\infsum}{\sum\limits_{i=1}^\infty}
\newcommand{\sets}{\{A_1, A_2 \dots\} }
\newcommand{\nsets}{\{A_1, \dots, A_n \} }

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\LARGE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\titleAT[CS 224N: Assignment 1]{Ryan McMahon}
\large

\begingroup
\let\clearpage\relax
\tableofcontents
\endgroup
\newpage

%-------------------------------------
\problem{1: Softmax (10 pts)}
%-------------------------------------

%----------------------
\subproblem{a}{Softmax Invariance to Constant (5 pts)}
\textit{Prove that softmax is invariant to constant offsets in the input, that is, for any input vector $x$ and any constant $c$, softmax($x$) = softmax($x + c$), where $x + c$ means adding the constant $c$ to every dimension of $x$. Remember that} 
\begin{equation}
	softmax(x)_{i} = \frac{e^{x_{i}}}{\sum_{j} e^{x_{j}}}
\end{equation}


\noindent\textbf{Answer:} \\

\noindent We can show that softmax($x$) = softmax($x+c$) by factoring out $c$ and canceling:

\begin{align}
	softmax(x + c)_{i} &= \frac{e^{x_{i} + c}}{\sum_{j} e^{x_{j} + c}} %
	 					= \frac{e^{x_{i}} \times e^{c}}{e^{c} \times \sum_{j} e^{x_{j}}} \nonumber \\
	%
					   &= \frac{e^{x_{i}} \times \cancel{e^{c}}}{\cancel{e^{c}} \times \sum_{j} e^{x_{j}}} %
					    = softmax(x)_{i} \nonumber
\end{align}

\vskip5em

%----------------------
\subproblem{b}{Softmax Coding (5 pts)}
\textit{Given an input matrix of $N$ rows and $D$ columns, compute the softmax prediction for each
row using the optimization in part (a). Write your implementation in} \verb|q1_softmax.py|. \textit{You may test
by executing} \verb|python q1_softmax.py|. \\

\noindent\textit{Note: The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is important to have a correct implementation. Your implementation should also be efficient and vectorized whenever possible (i.e., use numpy matrix operations rather than for loops). A non-vectorized implementation will not receive full credit!} \\

\noindent \textbf{Answer:} \\

\noindent See code: $\sim$\verb|/code/q1_softmax.py|.



\newpage
%-------------------------------------
\problem{2: Neural Network Basics (30 pts)}
%-------------------------------------


%----------------------
\subproblem{a}{Sigmoid Gradient (3 pts)}

\textit{Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expression where only Ïƒ(x), but not x, is present). Assume that the input x is a scalar for this question. Recall, the sigmoid function is}

\begin{equation}
	\sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}


\noindent \textbf{Answer:}

\begin{align}
	\sigma(x) &= \frac{1}{1 + e^{-x}} \nonumber \\
	%
			  &= \frac{e^{x}}{1 + e^{x}} \nonumber \\
	%
	\frac{\partial}{\partial x} \sigma(x) &= \frac{e^{x} \times (1 + e^{x}) - (e^{x} \times e^{x})}{(1 + e^{x})^{2}} \nonumber \\
	%
			  &= \frac{e^{x} + \cancel{(e^{x} \times e^{x})} - \cancel{(e^{x} \times e^{x})}}{(1 + e^{x})^{2}} \nonumber \\
	%
			  &= \frac{e^{x}}{(1 + e^{x})^{2}}  = \sigma(x) \times (1 - \sigma(x)) \nonumber 
\end{align}

\vskip2em
\noindent Because $1 - \sigma(x) = \sigma(-x)$ we can show that:

\begin{align}
	\frac{\partial}{\partial x} \sigma(x) &= \frac{e^{x}}{(1 + e^{x})^{2}} \nonumber \\
	%
			  &= \sigma(x) \times \sigma(-x) \nonumber \\
	%
			  &= \frac{e^{x}}{1 + e^{x}} \times \frac{1}{1 + e^{+x}} \nonumber \\
	%
			  &= \frac{e^{x}}{(1 + e^{x})^{2}} \nonumber
\end{align}



%----------------------
\newpage
\subproblem{b}{Softmax Gradient w/ Cross Entropy Loss (3 pts)}

\textit{Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation, i.e., find the gradients with respect to the softmax input vector $\bm{\theta}$, when the prediction is made by $\hat{\mathbf{y}} = softmax(\bm{\theta})$. Remember the cross entropy function is}

\begin{equation}
	CE(\mathbf{y},\hat{\mathbf{y}}) = - \sum_{i} y_{i} \times log(\hat{y_{i}})
\end{equation}

\noindent \textit{where $\mathbf{y}$ is the one-hot label vector, and $\hat{\mathbf{y}}$ is the predicted probability vector for all classes. (Hint: you might want to consider the fact many elements of $\mathbf{y}$ are zeros, and assume that only the $k-th$ dimension
of $\mathbf{y}$ is one.)} \\

\noindent \textbf{Answer:} \\

\noindent Let $S$ represent the softmax function:

\begin{align}
	f_{i} &= e^{\theta_{i}} \nonumber \\
	%
	g_{i} &= \sum_{k=1}^{K} e^{\theta_{k}} \nonumber \\
	%
	S_{i} &= \frac{f_{i}}{g_{i}} \nonumber \\
	%
	\frac{\partial S_{i}}{\partial \theta_{j}} &= \frac{f'_{i} g_{i} - g'_{i} f_{i}}{g_{i}^{2}} \nonumber
\end{align}

\vskip2em
%
\noindent So if $i = j$:
\begin{align}
	f'_{i} &= f_{i}; \hspace*{5pt} g'_{i} = e^{\theta_{j}} \nonumber \\
	%
	\frac{\partial S_{i}}{\partial \theta_{j}} &= \frac{e^{\theta_{i}} \sum_{k} e^{\theta_{k}} - e^{\theta_{j}} e^{\theta_{i}} }{ (\sum_{k} e^{\theta_{k}})^{2} } \nonumber \\
	%
		   &= \frac{e^{\theta_{i}}}{\sum_{k} e^{\theta_{k}}} \times \frac{\sum_{k} e^{\theta_{k}} - e^{\theta_{j}}}{\sum_{k} e^{\theta_{k}}} \nonumber \\
	%
		   &= S_{i} \times (1 - S_{i}) \nonumber
\end{align}

\vskip2em
%
\noindent And if $i \ne j$:
\begin{align}
	\frac{\partial S_{i}}{\partial \theta_{j}} %
		&= \frac{0 - e^{\theta_{j}} e^{\theta_{i}}}{(\sum_{k} e^{\theta_{k}})^{2}} \nonumber \\
	%
		&= - \frac{e^{\theta_{j}}}{\sum_{k} e^{\theta_{k}}} \times \frac{e^{\theta_{i}}}{\sum_{k} e^{\theta_{k}}} \nonumber \\
	%
		&= -S_{j} \times S_{i} \nonumber
\end{align}

\newpage

\noindent We can now use these when operating on our loss function (let $L$ represent the cross entropy function):
\begin{align}
	\frac{\partial L}{\partial \theta_{i}} % 
		&= - \sum_{k} y_{k} \frac{\partial log S_{k}}{\partial \theta_{i}}	\nonumber \\
	%
		&= - \sum_{k} y_{k} \frac{1}{S_{k}} \frac{\partial S_{k}}{\partial \theta_{i}} \nonumber \\
	%
		&= - y_{i} (1 - S_{i}) - \sum_{k \ne i} y_{k} \frac{1}{S_{k}} (-S_{k} \times S_{i}) \nonumber \\
	% 
		&= - y_{i} (1 - S_{i}) + \sum_{k \ne i} y_{k} S_{i} \nonumber \\
	%
		&= - y_{i} + y_{i} S_{i} + \sum_{k \ne i} y_{k} S_{i} \nonumber \\
	%
		&= S_{i}(\sum_{k} y_{k}) - y_{i} \nonumber
\end{align}

\vskip2em

\noindent And because we know that $\sum_{k} y_{k} = 1$:
\begin{align}
	\frac{\partial L}{\partial \theta_{i}} % 
			&= S_{i} - y_{i} \nonumber
\end{align} 



%----------------------
\newpage
\subproblem{c}{One Hidden Layer Gradient (6 pts)}

\textit{Derive the gradients with respect to the inputs $\bm{x}$ to a one-hidden-layer neural network (that is, find $\frac{\partial J}{\partial \bm{x}}$ where $J = CE(\mathbf{y}, \hat{\mathbf{y}})$ is the cost function for the neural network). The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is $\mathbf{y}$, and cross entropy cost is used. (Feel free to use $\sigma'(x)$ as the shorthand for sigmoid gradient, and feel free to define any variables whenever you see fit.)} \\

\noindent \textit{Recall that forward propoagation is as follows}
\begin{align}
	\mathbf{h} &= sigmoid(\bm{xW}_{1} + \bm{b}_{1})	& \hat{\bm{y}} = softmax(\bm{hW}_{2} + \bm{b}_{2}) \nonumber
\end{align}

\vskip2em

\noindent \textbf{Answer:} \\

\noindent Let $f_{2} = \bm{xW}_{1} + \bm{b}_{1}$ and $f_{3} = \bm{hW}_{2} + \bm{b}_{2}$;

\begin{align}
	\frac{\partial J}{\partial f_{3}} %
		&= \bm{\delta}_{3} = \hat{\bm{y}} - \bm{y} \nonumber \\
	%
	\frac{\partial J}{\partial \bm{h}} %
		&= \bm{\delta}_{2} = \bm{\delta}_{3}\bm{W}_{2}^{T} \nonumber \\
	%
	\frac{\partial J}{\partial f_{2}} %
			&= \bm{\delta}_{1} = \bm{\delta}_{2} \circ \sigma'(f_{2}) \nonumber \\
	%
	\frac{\partial J}{\partial \bm{x}} %
		&= \bm{\delta}_{1} \frac{\partial f_{2}}{\partial \bm{x}} \nonumber \\
	%
		&= \bm{\delta}_{1}  \bm{W}_{1}^{T} \nonumber
\end{align}

\vskip5em

%----------------------
\subproblem{d}{No. Parameters (2 pts)}

\textit{How many parameters are there in this neural network} [from (\textbf{c}) above], \textit{assuming the input is $D_{x}-$dimensional, the output is $D_{y}-$dimensional, and there are $H$ hidden units?} \\

\noindent \textbf{Answer:} \\

\begin{align}
	n_{W_{1}} &= D_{x} \times H \nonumber \\
	%
	n_{b_{1}} &= H \nonumber \\
	%
	n_{W_{2}} &= H \times D_{y} \nonumber \\
	%
	n_{b_{2}} &= D_{y} \nonumber \\
	%
	N &= (D_{x} \times H) + H + (H \times D_{y}) + D_{y} \nonumber
\end{align}



%----------------------
\newpage
\subproblem{e}{Sigmoid Activation Code (4 pts)}

\textit{Fill in the implementation for the sigmoid activation function and its gradient in} \verb|q2_sigmoid.py|. \textit{Test your implementation using} \verb|python q2_sigmoid.py|. \textit{Again, thoroughly test your code as the provided tests may not be exhaustive.} \\

\noindent \textbf{Answer:} \\

\noindent See code: $\sim$\verb|/code/q2_sigmoid.py|.

\vskip5em


%----------------------
\subproblem{f}{Gradient Check Code (4 pts)}

\textit{To make debugging easier, we will now implement a gradient checker. Fill in the implementation for} \verb|gradcheck_naive| \textit{in} \verb|q2_gradcheck.py|. \textit{Test your code using} \verb|python q2_gradcheck.py|. \\

\noindent \textbf{Answer:} \\

\noindent See code: $\sim$\verb|/code/q2_gradcheck.py|.

\vskip5em



\end{document}
